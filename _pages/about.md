---
permalink: /
title: "Susav Shrestha"
excerpt: "About me"
author_profile: true
# toc: true
# toc_sticky: true
redirect_from:
  - /about/
  - /about.html
---

<head>
  <meta name="description" content="Susav is a PhD candidate at Texas A&M University. His research focuses on building efficient and scalable machine learning systems with an emphasis on inference optimization.">
</head>

<!-- {% include toc %} -->

I specialize in building **efficient and scalable machine learning systems** for real-world deployment. My work bridges the gap between algorithm design and system-level efficiency, with a focus on **accelerating large-scale inference** through hardware-aware design, sparsity, and parallelism.

I am a PhD candidate in Computer Engineering at **Texas A&M University**, advised by [Dr. Narasimha Reddy](https://experts.tamu.edu/expert/narasimha-annapareddy/). 

---
### Research Interests

- Efficient and Sparse LLM Inference
- Hardware Efficient and High-Throughput Distributed Inference at Scale

### Selected Publications

- **Polar Sparsity**: High Throughput Batched LLM Inferencing with Scalable Contextual Sparsity
**Susav Shrestha**, Brad Settlemyer, Nikoli Dryden, Narasimha Reddy. [Paper](https://arxiv.org/abs/2505.14884),[Code](https://github.com/susavlsh10/Polar-Sparsity)  

- **ESPN**: Memory-Efficient Multi-vector Information Retrieval  
**Susav Shrestha**, Narasimha Reddy, Zongwang Li. In Proceedings of ISMM 2024[Paper](https://doi.org/10.1145/3652024.3665515),[Code](https://github.com/susavlsh10/ESPN-v1)

### Selected Experience

- Research Intern, NVIDIA, Santa Clara, CA, May - Aug 2025
- Research Intern, NVIDIA, Austin, TX, May - Aug 2024
- Research Intern, Samsung Semiconductor, San Jose, CA, May - Aug 2022  

### Updates
- ðŸ”¬ Started a **Research Internship at NVIDIA** for Summer 2025 in Santa Clara  
- ðŸ“„ Released new paper on [Polar Sparsity](https://arxiv.org/abs/2505.14884) with up to **2.2Ã— decoding speedup** in LLM inference  
